import streamlit as st
import sys
import os
import json
from datetime import datetime
import time
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Add project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import modules
from src.core.workflow import build_advanced_workflow
from src.core.models import AdvancedAgentState
from src.core.progress_tracker import ProgressTracker, StepStatus
from src.ui.progress_display import display_progress

# Page config
st.set_page_config(
    page_title="arXiv Research Agent (Lightweight)",
    page_icon="ğŸ“š",
    layout="wide"
)

# Initialize session state
if 'research_results' not in st.session_state:
    st.session_state.research_results = None

# Main UI
st.title("ğŸ“š arXiv Research Agent (è»½é‡ç‰ˆ)")
st.info("ğŸš€ é«˜é€Ÿç‰ˆï¼šPDFç¿»è¨³æ©Ÿèƒ½ã‚’é™¤ã„ãŸè»½é‡ç‰ˆã§ã™ã€‚æ¤œç´¢ãƒ»åˆ†æãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã®ã¿ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
st.divider()

# Research form
st.subheader("è«–æ–‡æ¤œç´¢ãƒ»åˆ†æ")

col1, col2 = st.columns([3, 1])

with col1:
    query = st.text_input(
        "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
        placeholder="ä¾‹: transformer architecture, reinforcement learning",
        help="èª¿æŸ»ã—ãŸã„ãƒˆãƒ”ãƒƒã‚¯ã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›"
    )

with col2:
    analysis_depth = st.selectbox(
        "åˆ†æã®æ·±ã•",
        ["shallow", "moderate", "deep"],
        index=1,
        format_func=lambda x: {
            "shallow": "ç°¡æ˜“åˆ†æ",
            "moderate": "æ¨™æº–åˆ†æ", 
            "deep": "è©³ç´°åˆ†æ"
        }[x]
    )

# Advanced options
with st.expander("è©³ç´°è¨­å®š"):
    thread_id = st.text_input("ã‚¹ãƒ¬ãƒƒãƒ‰ID", value=f"thread_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
    token_budget = st.number_input("ãƒˆãƒ¼ã‚¯ãƒ³äºˆç®—", min_value=10000, max_value=100000, value=30000)
    use_vertex_ai = st.checkbox("Vertex AIã‚’ä½¿ç”¨", value=True, help="ãƒã‚§ãƒƒã‚¯ã‚’å¤–ã™ã¨Google AI APIã‚’ä½¿ç”¨ã—ã¾ã™")

# Search button
if st.button("æ¤œç´¢ãƒ»åˆ†æé–‹å§‹", type="primary"):
    if query:
        # Create placeholder for progress display
        progress_placeholder = st.empty()
        
        try:
            # Update config based on user selection
            from src.core.config import get_model_config
            model_config = get_model_config()
            model_config.use_vertex_ai = use_vertex_ai
            
            # Show configuration being used
            with progress_placeholder.container():
                st.info(f"æ¤œç´¢ãƒ»åˆ†æã‚’é–‹å§‹ã—ã¦ã„ã¾ã™... (API: {'Vertex AI' if use_vertex_ai else 'Google AI'})")
            
            # Build workflow
            try:
                arxiv_agent = build_advanced_workflow()
            except Exception as e:
                st.error(f"ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
                logger.error(f"Failed to build workflow: {e}")
                st.stop()
            
            # Initial state
            initial_state = {
                "initial_query": query,
                "research_plan": None,
                "search_queries": [],
                "found_papers": [],
                "analyzed_papers": [],
                "final_report": "",
                "token_budget": token_budget,
                "analysis_mode": f"advanced_{analysis_depth}",
                "total_tokens_used": 0,
                "progress_tracker": None
            }
            
            # Run research
            with progress_placeholder.container():
                st.info("ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œä¸­...")
                st.caption(f"API: {'Vertex AI' if use_vertex_ai else 'Google AI'} | ãƒ¢ãƒ‡ãƒ«: {model_config.model_name}")
                st.warning("â³ åˆå›å®Ÿè¡Œæ™‚ã¯10-20ç§’ç¨‹åº¦ã‹ã‹ã‚Šã¾ã™ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„...")
                
                progress_bar = st.progress(0)
                status_text = st.empty()
            
            # Log before invoke
            logger.info(f"Starting workflow invoke with query: {query}")
            invoke_start_time = time.time()
            
            try:
                result = arxiv_agent.invoke(initial_state)
                invoke_time = time.time() - invoke_start_time
                logger.info(f"Workflow completed in {invoke_time:.1f} seconds")
            except Exception as e:
                invoke_time = time.time() - invoke_start_time
                logger.error(f"Workflow failed after {invoke_time:.1f} seconds: {e}")
                with progress_placeholder.container():
                    st.error(f"ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
                    st.caption(f"å®Ÿè¡Œæ™‚é–“: {invoke_time:.1f}ç§’")
                raise
            
            # Display final progress
            if result.get("progress_tracker"):
                tracker_data = result["progress_tracker"]
                tracker = ProgressTracker()
                
                # Reconstruct tracker from data
                tracker.start_time = tracker_data.get("start_time", tracker.start_time)
                tracker.is_complete = tracker_data.get("is_complete", False)
                tracker.error_occurred = tracker_data.get("error_occurred", False)
                
                # Restore step states
                for step_name, step_data in tracker_data.get("steps", {}).items():
                    if step_name in tracker.steps:
                        step = tracker.steps[step_name]
                        step.status = StepStatus(step_data.get("status", "pending"))
                        step.start_time = step_data.get("start_time")
                        step.end_time = step_data.get("end_time")
                        step.current_item = step_data.get("current_item")
                        step.total_items = step_data.get("total_items", 0)
                        step.completed_items = step_data.get("completed_items", 0)
                        step.error_message = step_data.get("error_message")
                        step.details = step_data.get("details", {})
                
                progress_placeholder.empty()
                with progress_placeholder.container():
                    display_progress(tracker)
            else:
                progress_placeholder.empty()
            
            # Store results
            st.session_state.research_results = result
            st.success("åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            
        except Exception as e:
            progress_placeholder.empty()
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    else:
        st.warning("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

# Display results
if st.session_state.research_results:
    results = st.session_state.research_results
    
    st.divider()
    st.subheader("åˆ†æçµæœ")
    
    # Summary metrics
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("åˆ†æè«–æ–‡æ•°", len(results.get('analyzed_papers', [])))
    with col2:
        total_tokens = sum(p.get('tokens_used', 0) for p in results.get('analyzed_papers', []))
        st.metric("ç·ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡", f"{total_tokens:,}")
    with col3:
        st.metric("æ¤œç´¢ã‚¯ã‚¨ãƒªæ•°", len(results.get('search_queries', [])))
    
    # Final report
    if results.get('final_report'):
        st.subheader("ğŸ“‹ åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        st.markdown(results['final_report'])
    
    # Papers
    st.subheader("ğŸ“„ åˆ†æã•ã‚ŒãŸè«–æ–‡")
    
    for i, paper in enumerate(results.get('analyzed_papers', [])):
        with st.expander(f"{i+1}. {paper['metadata']['title']}", expanded=False):
            # Metadata
            col1, col2 = st.columns([2, 1])
            with col1:
                st.write(f"**è‘—è€…**: {', '.join(paper['metadata']['authors'])}")
                st.write(f"**å…¬é–‹æ—¥**: {paper['metadata']['published']}")
            with col2:
                st.write(f"**ã‚«ãƒ†ã‚´ãƒª**: {', '.join(paper['metadata']['categories'])}")
                coverage = paper.get('coverage', {})
                avg_coverage = sum(coverage.values()) / max(len(coverage), 1) * 100
                st.write(f"**ã‚«ãƒãƒ¬ãƒƒã‚¸**: {avg_coverage:.1f}%")
            
            # Summary
            st.write("**è¦ç´„**")
            st.write(paper['metadata']['summary'])
            
            # Analysis
            if 'analysis' in paper:
                st.write("**åˆ†æçµæœ**")
                st.json(paper['analysis'])
            
            # PDF link
            if 'pdf_url' in paper['metadata']:
                st.markdown(f"[ğŸ“„ PDF ã‚’é–‹ã]({paper['metadata']['pdf_url']})")

# Footer
st.divider()
st.caption("arXiv Research Agent (è»½é‡ç‰ˆ) - é«˜é€Ÿè«–æ–‡æ¤œç´¢ãƒ»åˆ†æãƒ„ãƒ¼ãƒ«")

# Show restoration info
with st.expander("ğŸ’¡ å®Œå…¨ç‰ˆã¸ã®å¾©å…ƒæ–¹æ³•"):
    st.markdown("""
    **PDFç¿»è¨³æ©Ÿèƒ½ä»˜ãå®Œå…¨ç‰ˆã«æˆ»ã™ã«ã¯ï¼š**
    
    ```bash
    # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å¾©å…ƒ
    cp backup/workflow_with_translation.py src/core/workflow.py
    
    # ã‚¢ãƒ—ãƒªã‚’å¾©å…ƒ
    cp backup/app_with_translation.py app.py
    ```
    
    **è»½é‡ç‰ˆã®åˆ©ç‚¹ï¼š**
    - âš¡ å‡¦ç†é€Ÿåº¦ãŒå¤§å¹…å‘ä¸Š
    - ğŸ’° APIä½¿ç”¨é‡ã‚’å‰Šæ¸›
    - ğŸ¯ æ¤œç´¢ãƒ»åˆ†æã«ç‰¹åŒ–
    """)